{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "##The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OI_L93eYy-D",
        "outputId": "9596d687-e898-447c-fff5-6132e95237c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "  print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icwOrUg2XwZk",
        "outputId": "604c8b84-6557-475a-9d48-ff75d5f43635"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "DRIVE_PATH = './drive/MyDrive/DynamicResNet'\n",
        "DROPOUT_LOC = 'LastLayer'"
      ],
      "metadata": {
        "id": "N93l9M4SbvSE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "#**CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2Vn2qhvrIi"
      },
      "source": [
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5WmV1je_1kr",
        "outputId": "acd8bdbd-00d6-46d2-f88c-f38caa89073f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "print('==> Preparing data..')\n",
        "#Image augmentation is used to train the model\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "#Only the data is normalaized we do not need to augment the test data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform_train)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform_test)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqQ7fsUxA0r"
      },
      "source": [
        "#**Defining the Network Architecture**\n",
        "In this section the entire Research Paper is implemented to define the Residual Network approach taken by the researchers\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Output volume for a convolutional layer\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from Stanford's cs231n course):\n",
        "\n",
        "We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by (Wâˆ’F+2P)/S+1.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEc93C7xAXEE"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Dropout\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BottleNeck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes , planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes :\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = F.relu(self.bn2(self.conv2(out)))\n",
        "    out = self.bn3(self.conv3(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, dropout_rate, dropout_location, logger_path, num_classes=10, curr_epoch=0):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    #### variables we added\n",
        "    \"\"\"\n",
        "    :param dropout_location: \"last\", \"middle\", \"all\"\n",
        "    \"\"\"\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.dropout_location = dropout_location\n",
        "    self.curr_epoch = curr_epoch\n",
        "    ####\n",
        "\n",
        "    #### logger\n",
        "    # self.logger = logging.getLogger(__name__)\n",
        "    # self.logger.setLevel(logging.DEBUG)\n",
        "    \n",
        "    # formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # fh = logging.FileHandler(logger_path)\n",
        "    # fh.setLevel(logging.DEBUG)\n",
        "    # fh.setFormatter(formatter)\n",
        "    # self.logger.addHandler(fh)\n",
        "    ####\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    if self.dropout_location == 'all':\n",
        "      self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, is_dropout=True)\n",
        "      self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, is_dropout=True)\n",
        "      self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, is_dropout=True)\n",
        "      self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_dropout=True)\n",
        "    else:\n",
        "      self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, is_dropout=False)\n",
        "      self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, is_dropout=False)\n",
        "      if self.dropout_location == 'middle': \n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, is_dropout=True)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_dropout=False)\n",
        "      else: # self.dropout_location == 'last':\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, is_dropout=False)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_dropout=True)      \n",
        "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "  def get_dropout_rate(self):\n",
        "      return self.dropout_rate\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride, is_dropout):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    if is_dropout:\n",
        "      return nn.Sequential(*layers, Dropout(self.dropout_rate))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "\n",
        "def ResNet18(rate, location, path):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], dropout_rate=rate, dropout_location=location, logger_path=path)\n",
        "\n",
        "def ResNet34(rate, location, path):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], dropout_rate=rate, dropout_location=location, logger_path=path)\n",
        "\n",
        "def ResNet50(rate, location, path):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], dropout_rate=rate, dropout_location=location, logger_path=path)\n",
        "\n",
        "def ResNet101(rate, location, path):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], dropout_rate=rate, dropout_location=location, logger_path=path)\n",
        "\n",
        "def ResNet152(rate, location, path):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], dropout_rate=rate, dropout_location=location, logger_path=path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.0\n",
        "dropout_location = 'last'\n",
        "logger_path = f\"{DRIVE_PATH}/example.log\"\n",
        "net = ResNet18(dropout_rate, dropout_location, logger_path)\n",
        "\n",
        "print(net)\n",
        "\n",
        "if train_on_gpu:\n",
        "  net = torch.nn.DataParallel(net)\n",
        "  cudnn.benchmark = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwLdvrIQZV5F",
        "outputId": "61efb9ae-4058-4caf-9382-3939ea556924"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAlhqzyyYaG"
      },
      "source": [
        "#**Specifying the Loss Function and Optimizer**\n",
        "We use CrossEntropyLoss as Loss function and\n",
        "\n",
        "[Stochastic Gradient Descent](https://leon.bottou.org/publications/pdf/compstat-2010.pdf) as Optimizer with momentum and weight decay specified by the research paper of ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlkXUNc-aVV9"
      },
      "source": [
        "import torch.optim as optim\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch, model, optimizer, loss):\n",
        "    \"\"\"Saves model checkpoint\"\"\"\n",
        "    torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss.state_dict(),\n",
        "                'dropout':model.get_dropout_rate(),\n",
        "                }, f'{DRIVE_PATH}/{DROPOUT_LOC}/ResNet18_{model.get_dropout_rate()}.pth')"
      ],
      "metadata": {
        "id": "FoZrTwlUXjWX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Load model checkpoint\n",
        "def load_checkpoint(path, dropout_location, logger_path):\n",
        "  try:\n",
        "    checkpoint = torch.load(path)\n",
        "    curr_epoch = checkpoint.get('epoch',0)\n",
        "    dropout_rate = checkpoint.get('dropout',0.0)\n",
        "    net = ResNet18(dropout_rate=dropout_rate ,curr_epoch=curr_epoch+1, dropout_location=dropout_location, logger_path=logger_path)\n",
        "    #### Load model and optimizer state dictionaries\n",
        "    net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criterion.load_state_dict(checkpoint['loss'])\n",
        "    return net\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "rzOnkzmqXlPW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_map = {}\n",
        "for dropout_val in [0.0,0.1,0.2,0.3,0.4,0.5]:\n",
        "  net_path = f'{DRIVE_PATH}/{DROPOUT_LOC}/ResNet18_{dropout_val}'\n",
        "  logger_path = f\"{net_path}.log\"\n",
        "  net = load_checkpoint(f'{net_path}.pth', \n",
        "                        dropout_location=dropout_location, logger_path=logger_path)\n",
        "  if net is None:\n",
        "    net = ResNet18(dropout_val, dropout_location, logger_path)\n",
        "    # net.apply(weights_init) ??????????????\n",
        "  else:\n",
        "    print(\"loaded model from disk\")  \n",
        "  if train_on_gpu:\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "  model_map[dropout_val] = net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI2ndzWuYOgW",
        "outputId": "1252557d-3f3d-4c08-de7a-923a408848e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './drive/MyDrive/DynamicResNet/LastLayer/ResNet18_0.0.pth'\n",
            "[Errno 2] No such file or directory: './drive/MyDrive/DynamicResNet/LastLayer/ResNet18_0.1.pth'\n",
            "[Errno 2] No such file or directory: './drive/MyDrive/DynamicResNet/LastLayer/ResNet18_0.2.pth'\n",
            "[Errno 2] No such file or directory: './drive/MyDrive/DynamicResNet/LastLayer/ResNet18_0.3.pth'\n",
            "[Errno 2] No such file or directory: './drive/MyDrive/DynamicResNet/LastLayer/ResNet18_0.4.pth'\n",
            "[Errno 2] No such file or directory: './drive/MyDrive/DynamicResNet/LastLayer/ResNet18_0.5.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLPoZuGAcUM8",
        "outputId": "206b9a9f-33ac-48ec-a055-5658e763d03b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 1\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  # keep track of training and validation loss\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "    \n",
        "  ###################\n",
        "  # train the model #\n",
        "  ###################\n",
        "  net.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = net(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # backward pass: compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "    # perform a single optimization step (parameter update)\n",
        "    optimizer.step()\n",
        "    # update training loss\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "  ######################    \n",
        "  # validate the model #\n",
        "  ######################\n",
        "  net.eval()\n",
        "  for batch_idx, (data, target) in enumerate(valid_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = net(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update average validation loss \n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "  # calculate average losses\n",
        "  train_loss = train_loss/len(train_loader.sampler)\n",
        "  valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "  epoch_info = 'Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss)      \n",
        "  # print training/validation statistics \n",
        "  print(epoch_info)\n",
        "\n",
        "\n",
        "  logging.info(epoch_info)\n",
        "\n",
        "  # save model if validation loss has decreased\n",
        "  if valid_loss <= valid_loss_min:\n",
        "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "    valid_loss_min,\n",
        "    valid_loss))\n",
        "    torch.save(net.state_dict(), 'ResNet18.pt')\n",
        "    valid_loss_min = valid_loss"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 2.407289 \tValidation Loss: 2.401601\n",
            "Validation loss decreased (inf --> 2.401601).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1M4p3Is2RMg"
      },
      "source": [
        "#**Loading the Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "angPoGSTeykj"
      },
      "source": [
        "net.load_state_dict(torch.load('ResNet18.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lEna0p2Wp5"
      },
      "source": [
        "#**Testing Loop**\n",
        "The real test of the model architecture how well does the model recognizes the image and what is the accuracy on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsqLF8SVfsnn"
      },
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for batch_idx, (data, target) in enumerate(test_loader):\n",
        "  # move tensors to GPU if CUDA is available\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  # forward pass: compute predicted outputs by passing inputs to the model\n",
        "  output = net(data)\n",
        "  # calculate the batch loss\n",
        "  loss = criterion(output, target)\n",
        "  # update test loss \n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, pred = torch.max(output, 1)    \n",
        "  # compare predictions to true label\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  # calculate test accuracy for each object class\n",
        "  for i in range(batch_size):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "  if class_total[i] > 0:\n",
        "    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i],\n",
        "        np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "  else:\n",
        "    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8r7sNDiWwie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}