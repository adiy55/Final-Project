{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "##The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OI_L93eYy-D",
        "outputId": "c73b4af3-1ade-45ac-f02f-d629111fda71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "  print('CUDA is available!  Training on GPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icwOrUg2XwZk",
        "outputId": "d43897de-ad26-4d9c-d57d-6ce2d56f029e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N93l9M4SbvSE"
      },
      "outputs": [],
      "source": [
        "DRIVE_PATH = './drive/MyDrive/DynamicResNet'\n",
        "\n",
        "log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PLsgon_uX7G",
        "outputId": "fa0d6eda-a9dd-46e0-f2d8-9cd8cbf2459e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:test message\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import logging.handlers\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "                    handlers=[logging.StreamHandler()])\n",
        "\n",
        "\n",
        "logging.info(\"test message\")\n",
        "logging.error(\"test message\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q6m-FNKZzEZ"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create the memory handler with a capacity of 1000 records\n",
        "mem_handler = logging.handlers.MemoryHandler(capacity=1000)\n",
        "\n",
        "# Add the memory handler to the logger\n",
        "logger.addHandler(mem_handler)\n",
        "\n",
        "# Create a file handler that will flush the memory handler to a file every 30 seconds\n",
        "file_handler = logging.FileHandler(f'{DRIVE_PATH}/output.log')\n",
        "file_handler.setLevel(logging.DEBUG)\n",
        "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
        "\n",
        "# Add the file handler to the logger\n",
        "logger.addHandler(file_handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqyq8M1rZeHa",
        "outputId": "963b7d73-561d-4dfa-a838-f28383032e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:__main__:Test Message 0\n",
            "DEBUG:__main__:Test Message 1\n"
          ]
        }
      ],
      "source": [
        "# Log some messages\n",
        "for i in range(2):\n",
        "    logger.debug(f\"Test Message {i}\")\n",
        "\n",
        "# Flush the memory handler to the file\n",
        "mem_handler.flush()\n",
        "\n",
        "# Close the file handler\n",
        "file_handler.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2VFiNdnRWdH"
      },
      "outputs": [],
      "source": [
        "# Save the initial resnet model (THESE ARE THE WEIGHTS USED TO INITALIZE ALL RESNET18 MODELS!)\n",
        "INITIAL_NET_PATH = os.path.join(DRIVE_PATH,\"ResNet18_initial.pth\")\n",
        "def save_initial_net(initial_net_model):\n",
        "  if not os.path.exists(INITIAL_NET_PATH):\n",
        "    torch.save(initial_net_model.state_dict(), f'{DRIVE_PATH}/ResNet18_initial.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "#**CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2Vn2qhvrIi"
      },
      "source": [
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5WmV1je_1kr",
        "outputId": "487f201a-106d-40ee-ddc3-6d44547943a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13232642.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "print('==> Preparing data..')\n",
        "#Image augmentation is used to train the model\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "#Only the data is normalaized we do not need to augment the test data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform_train)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform_test)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "train_idx = indices\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers,shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqQ7fsUxA0r"
      },
      "source": [
        "#**Defining the Network Architecture**\n",
        "In this section the entire Research Paper is implemented to define the Residual Network approach taken by the researchers\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Output volume for a convolutional layer\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from Stanford's cs231n course):\n",
        "\n",
        "We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by (W−F+2P)/S+1.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEc93C7xAXEE"
      },
      "outputs": [],
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Dropout\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BottleNeck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes , planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes :\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = F.relu(self.bn2(self.conv2(out)))\n",
        "    out = self.bn3(self.conv3(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, dropout_rate, dropout_location, model_architecture, num_classes=10, curr_epoch=0, valid_loss_min=np.Inf):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    #### variables we added\n",
        "    \"\"\"\n",
        "    :param dropout_location: \"last\", \"middle\", \"all\"\n",
        "    \"\"\"\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.dropout_location = dropout_location\n",
        "    self.curr_epoch = curr_epoch\n",
        "    ####\n",
        "\n",
        "    #### logger\n",
        "    self.model_props = f\"{model_architecture}.{dropout_location}.{dropout_rate}\"\n",
        "    ####\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    if self.dropout_location == 'all':\n",
        "      self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, is_dropout=True)\n",
        "      self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, is_dropout=True)\n",
        "      self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, is_dropout=True)\n",
        "      self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_dropout=True)\n",
        "    else:\n",
        "      self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, is_dropout=False)\n",
        "      self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, is_dropout=False)\n",
        "      if self.dropout_location == 'middle': \n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, is_dropout=True)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_dropout=False)\n",
        "      else: # self.dropout_location == 'last':\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, is_dropout=False)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_dropout=True)      \n",
        "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "  def get_dropout_rate(self):\n",
        "      return self.dropout_rate\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride, is_dropout):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    if is_dropout:\n",
        "      return nn.Sequential(*layers, Dropout(self.dropout_rate))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "\n",
        "def ResNet18(rate, location):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], model_architecture=\"ResNet18\", dropout_rate=rate, dropout_location=location)\n",
        "\n",
        "def ResNet34(rate, location):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], model_architecture=\"ResNet34\", dropout_rate=rate, dropout_location=location)\n",
        "\n",
        "def ResNet50(rate, location):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], model_architecture=\"ResNet50\", dropout_rate=rate, dropout_location=location)\n",
        "\n",
        "def ResNet101(rate, location):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], model_architecture=\"ResNet101\", dropout_rate=rate, dropout_location=location)\n",
        "\n",
        "def ResNet152(rate, location):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], model_architecture=\"ResNet152\", dropout_rate=rate, dropout_location=location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn9uZybz1Nzt"
      },
      "outputs": [],
      "source": [
        "# This class Wraps the torch.nn.DataParallel and adds the logging capability\n",
        "class LoggerDataParallel(torch.nn.DataParallel):\n",
        "    def __init__(self, module, model_props=None, *args, **kwargs):\n",
        "        super(LoggerDataParallel, self).__init__(module, *args, **kwargs)\n",
        "        self.model_props = model_props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwue19KBXdX6",
        "outputId": "eae8bf7e-1fb2-4734-b18c-b3bc740f1a0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('last', 0.0),\n",
              " ('middle', 0.0),\n",
              " ('last', 0.1),\n",
              " ('middle', 0.1),\n",
              " ('last', 0.2),\n",
              " ('middle', 0.2),\n",
              " ('last', 0.3),\n",
              " ('middle', 0.3),\n",
              " ('last', 0.4),\n",
              " ('middle', 0.4),\n",
              " ('last', 0.5),\n",
              " ('middle', 0.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dropout_values = [0.0,0.1,0.2,0.3,0.4,0.5]\n",
        "droput_locations = ['last','middle']\n",
        "combinations = []\n",
        "\n",
        "for drop_val in dropout_values:\n",
        "  for drop_loc in droput_locations:\n",
        "    combinations.append((drop_loc, drop_val))\n",
        "\n",
        "combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoZrTwlUXjWX"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, dropout_rate, dropout_location, train_loss_min ,suffix):\n",
        "    \"\"\"Saves model checkpoint\"\"\"\n",
        "    torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'dropout':dropout_rate,\n",
        "                'train_loss_min': train_loss_min,\n",
        "                }, f'{DRIVE_PATH}/{dropout_location}/ResNet18_{dropout_rate}_{suffix}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxmqrZ3yMiMs"
      },
      "outputs": [],
      "source": [
        "def print_accuracy_info(class_total, class_correct, prefix):\n",
        "    for i in range(10):\n",
        "      if class_total[i] > 0:\n",
        "        message = '%s Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            prefix, classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i]))\n",
        "        logger.info(message)\n",
        "        \n",
        "      else:\n",
        "        print('%s Accuracy of %5s: N/A (no training examples)' % (prefix, classes[i]))\n",
        "\n",
        "    message = '%s Accuracy (Overall): %2d%% (%2d/%2d)' % (prefix,\n",
        "        100. * np.sum(class_correct) / np.sum(class_total),\n",
        "        np.sum(class_correct), np.sum(class_total))\n",
        "    logger.info(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLPoZuGAcUM8"
      },
      "outputs": [],
      "source": [
        "def run_train_loop(curr_net, curr_optimizer, criterion, dropout_location, dropout_rate, curr_epoch = 0, train_loss_min = np.inf):\n",
        "  # number of epochs to train the model\n",
        "  n_epochs = 50\n",
        "\n",
        "  class_correct_train = list(0. for i in range(10))\n",
        "  class_total_train = list(0. for i in range(10))\n",
        "\n",
        "  for epoch in range(curr_epoch, n_epochs+1):\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "      \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    curr_net.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "      # clear the gradients of all optimized variables\n",
        "      curr_optimizer.zero_grad()\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = curr_net(data)\n",
        "      # calculate the batch loss\n",
        "      loss = criterion(output, target)\n",
        "      # backward pass: compute gradient of the loss with respect to model parameters\n",
        "      loss.backward()\n",
        "      # perform a single optimization step (parameter update)\n",
        "      curr_optimizer.step()\n",
        "      # update training loss\n",
        "      train_loss += loss.item()*data.size(0)\n",
        "\n",
        "      _, pred = torch.max(output, 1)    \n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      # calculate test accuracy for each object class\n",
        "      for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct_train[label] += correct[i].item()\n",
        "        class_total_train[label] += 1\n",
        "          \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    \n",
        "\n",
        "    epoch_info = 'Epoch: {} \\tTraining Loss: {:.6f} \\t'.format(epoch, train_loss)      \n",
        "    # print training/validation statistics \n",
        "    text_to_log = f\"{curr_net.model_props}, {epoch_info}\"\n",
        "    logger.info(text_to_log)\n",
        "\n",
        "    print_accuracy_info(class_total_train, class_correct_train, \"Train\")\n",
        "    if train_loss < train_loss_min:\n",
        "          train_loss_min = train_loss\n",
        "          \n",
        "          (epoch, curr_net, curr_optimizer, dropout_rate, dropout_location, train_loss_min,\"best_valid\")\n",
        "    save_checkpoint(epoch, curr_net, curr_optimizer, dropout_rate, dropout_location, train_loss_min ,\"last\")\n",
        "\n",
        "    # Flush the memory handler to the file\n",
        "    mem_handler.flush()\n",
        "\n",
        "    # Close the file handler\n",
        "    file_handler.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_92PMYNXVzW"
      },
      "outputs": [],
      "source": [
        "net = ResNet18(0.0, 'last')\n",
        "if train_on_gpu:\n",
        "  # net = torch.nn.DataParallel(net).cuda()\n",
        "  net = LoggerDataParallel(net, model_props = net.model_props)\n",
        "  cudnn.benchmark = True\n",
        "# net.get_dropout_rate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e45lMqquUwY0"
      },
      "outputs": [],
      "source": [
        "#### Load model checkpoint\n",
        "def load_checkpoint(path, dropout_location):\n",
        "  try:\n",
        "    checkpoint = torch.load(path)\n",
        "    model_state_dict_pre_fix = checkpoint.get(\"model_state_dict\")\n",
        "    shaved_state_dict = {}\n",
        "    for full_key in model_state_dict_pre_fix.keys():\n",
        "      if \"module\" in full_key:\n",
        "        shaved_state_dict[\".\".join(full_key.split(\".\")[1:])] = model_state_dict_pre_fix.get(full_key)\n",
        "    model_state_dict_pre_fix.update(shaved_state_dict)\n",
        "    curr_epoch = checkpoint.get('epoch', 0)\n",
        "    dropout_rate = checkpoint.get('dropout', 0.0)\n",
        "    train_loss_min = checkpoint.get('train_loss_min', np.inf)\n",
        "    net = ResNet18(dropout_rate, dropout_location)\n",
        "    #### Load model and optimizer state dictionaries\n",
        "    net.load_state_dict(shaved_state_dict)\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return net , curr_epoch, train_loss_min\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None, 0, np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVx90zYIX1U_"
      },
      "outputs": [],
      "source": [
        "logger_path = f\"{DRIVE_PATH}\"\n",
        "initial_net = ResNet18(0.0, 'last')\n",
        "state_dict = initial_net.state_dict()\n",
        "save_initial_net(initial_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXk5nNPfUQlm",
        "outputId": "33a625c4-66f5-46c1-a012-fe5fc24d4c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n",
            "loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "model_map = {}\n",
        "for combination in combinations:\n",
        "  dropout_location, dropout_rate = combination\n",
        "  \n",
        "  net_path = os.path.join(DRIVE_PATH,dropout_location,f\"ResNet18_{dropout_rate}_last.pth\")\n",
        "  net , curr_epoch, train_loss_min = load_checkpoint(net_path, dropout_location=dropout_location)\n",
        "\n",
        "  if net is None:\n",
        "    net = ResNet18(dropout_rate, dropout_location)\n",
        "    net.load_state_dict(copy.deepcopy(state_dict))\n",
        "    print(f\"Created the model using initial weights: {net.model_props}\")\n",
        "  else:\n",
        "    print(\"loaded model from disk\")  \n",
        "  # if train_on_gpu:\n",
        "  #   net = LoggerDataParallel(net, model_props = net.model_props)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "  model_key = f\"{dropout_location}_{dropout_rate}\"\n",
        "  model_map[model_key] = (net, curr_epoch, train_loss_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF-h9dlgWNzW"
      },
      "outputs": [],
      "source": [
        "def get_model_for_training(dropout_location, dropout_rate):\n",
        "  model_key = f\"{dropout_location}_{dropout_rate}\"\n",
        "  net, curr_epoch, train_loss_min = model_map.get(model_key)\n",
        "  print(f\"the following model {net.model_props} is starting from epoch : {curr_epoch}\")\n",
        "  return net, curr_epoch, train_loss_min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "TsV1d_LRYXpl",
        "outputId": "de3611e1-398b-415f-9e84-5a5a8e4e2d91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:ResNet18.last.0.0 Starting Training\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the following model ResNet18.last.0.0 is starting from epoch : 50\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ca7144b767ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mrun_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-550aab14ce4b>\u001b[0m in \u001b[0;36mrun_train_loop\u001b[0;34m(curr_net, curr_optimizer, criterion, dropout_location, dropout_rate, curr_epoch, train_loss_min)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mcurr_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# update training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for combination in combinations:\n",
        "  dropout_location, dropout_rate = combination\n",
        "  net, curr_epoch, train_loss_min = get_model_for_training(dropout_location, dropout_rate)\n",
        "\n",
        "  if curr_epoch == 0:\n",
        "    assert torch.all(torch.eq(initial_net.state_dict()['conv1.weight'], net.state_dict()['conv1.weight']))\n",
        "\n",
        "  if train_on_gpu:\n",
        "    net = LoggerDataParallel(net, model_props = net.model_props)\n",
        "    cudnn.benchmark = True\n",
        "  \n",
        "  text_to_log = f\"{net.model_props} Starting Training\"\n",
        "  logger.info(text_to_log)\n",
        "\n",
        "  # specify loss function (categorical cross-entropy)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # specify optimizer\n",
        "  optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "  run_train_loop(net, optimizer, criterion, dropout_location, dropout_rate, curr_epoch, train_loss_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ShQQZftdD0S"
      },
      "outputs": [],
      "source": [
        "# Flush the memory handler to the file\n",
        "mem_handler.flush()\n",
        "\n",
        "# Close the file handler\n",
        "file_handler.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1M4p3Is2RMg"
      },
      "source": [
        "#**Loading the Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofnr9nTCE8RO"
      },
      "outputs": [],
      "source": [
        "def enable_dropout(mod: nn.Module):\n",
        "    if isinstance(mod, nn.Dropout):\n",
        "        mod.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmHp3KBPGEzB",
        "outputId": "27c31caa-11aa-4570-cfdd-2e8c85e45fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['last_0.0', 'middle_0.0', 'last_0.1', 'middle_0.1', 'last_0.2', 'middle_0.2', 'last_0.3', 'middle_0.3', 'last_0.4', 'middle_0.4', 'last_0.5', 'middle_0.5'])\n",
            "the following model ResNet18.middle.0.5 is starting from epoch : 50\n",
            "50\n"
          ]
        }
      ],
      "source": [
        "print(model_map.keys())\n",
        "\n",
        "net, curr_epoch, train_loss_min = get_model_for_training('middle', 0.5)\n",
        "print(curr_epoch)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lEna0p2Wp5"
      },
      "source": [
        "#**Testing Loop**\n",
        "The real test of the model architecture how well does the model recognizes the image and what is the accuracy on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsqLF8SVfsnn"
      },
      "outputs": [],
      "source": [
        "# track test loss\n",
        "def run_test_data(net, optimizer, criterion, keep_dropout = True, is_train_loader = False):\n",
        "  test_loss = 0.0\n",
        "  class_correct = list(0. for i in range(10))\n",
        "  class_total = list(0. for i in range(10))\n",
        "\n",
        "  net.eval()\n",
        "  if keep_dropout:\n",
        "    net.apply(enable_dropout)\n",
        "\n",
        "  if train_on_gpu:\n",
        "      net = LoggerDataParallel(net, model_props = net.model_props)\n",
        "\n",
        "  curr_loader = train_loader if is_train_loader else test_loader\n",
        "  log_label = \"Train\" if is_train_loader else \"Test\"\n",
        "\n",
        "  # iterate over test data\n",
        "  for batch_idx, (data, target) in enumerate(curr_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = net(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "      label = target.data[i]\n",
        "      class_correct[label] += correct[i].item()\n",
        "      class_total[label] += 1\n",
        "\n",
        "  # average test loss\n",
        "  test_loss = test_loss/len(test_loader.dataset)\n",
        "  print_accuracy_info(class_total, class_correct, log_label)\n",
        "  \n",
        "  return np.sum(class_correct) / np.sum(class_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tA_JeEFjTc7"
      },
      "outputs": [],
      "source": [
        "results_dict_location = os.path.join(DRIVE_PATH, 'trial_results.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt3w8LzXjlIt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def save_trials_dict(trial_result_dict):\n",
        "  with open(results_dict_location, 'w') as f:\n",
        "      # write the dictionary to the file as JSON\n",
        "      json.dump(trial_result_dict, f)\n",
        "\n",
        "def load_trials_dict():\n",
        "  if os.path.exists(results_dict_location):\n",
        "    with open(results_dict_location, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZrFdhddw5jJ"
      },
      "outputs": [],
      "source": [
        "TRIALS_NUMBER = 20\n",
        "loaded_dict = load_trials_dict()\n",
        "trial_result_dict = {} if loaded_dict is None else copy.deepcopy(loaded_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa6HnKiDzNVz",
        "outputId": "eb10442b-9eea-4f9c-e81e-ed15ce7626dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['0.0_last', '0.0_middle', '0.1_last', '0.1_middle', '0.2_last', '0.2_middle', '0.3_last', '0.3_middle', '0.4_last', '0.4_middle', '0.5_last', '0.5_middle'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "trial_result_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1JWR-jZ7a-0"
      },
      "source": [
        "General schema of the results\n",
        "```\n",
        "{\n",
        "\n",
        "  \"0.4_last\":{\n",
        "  \n",
        "  \"0.0_train\": [1,2,3,.....],\n",
        "\n",
        "  \"0.0_test\": [1,2,3,.....]\n",
        "\n",
        "  },\n",
        "\n",
        "\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8r7sNDiWwie",
        "outputId": "aba9dc2c-faca-4ab0-9cba-8ad2b1bf35e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the following model ResNet18.last.0.0 is starting from epoch : 50\n",
            "Current Loss 0.0868239977533929 \n",
            "Loading into a new model with 0.0 dropout in last location\n",
            "Loading into a new model with 0.1 dropout in last location\n",
            "Loading into a new model with 0.2 dropout in last location\n",
            "Loading into a new model with 0.30000000000000004 dropout in last location\n",
            "Loading into a new model with 0.4 dropout in last location\n",
            "Loading into a new model with 0.5 dropout in last location\n",
            "Loading into a new model with 0.6000000000000001 dropout in last location\n",
            "Loading into a new model with 0.7000000000000001 dropout in last location\n",
            "Loading into a new model with 0.8 dropout in last location\n",
            "Loading into a new model with 0.9 dropout in last location\n",
            "Loading into a new model with 1.0 dropout in last location\n",
            "the following model ResNet18.middle.0.0 is starting from epoch : 50\n",
            "Current Loss 0.08586716080633923 \n",
            "Loading into a new model with 0.0 dropout in middle location\n",
            "Loading into a new model with 0.1 dropout in middle location\n",
            "Loading into a new model with 0.2 dropout in middle location\n",
            "Loading into a new model with 0.30000000000000004 dropout in middle location\n",
            "Loading into a new model with 0.4 dropout in middle location\n",
            "Loading into a new model with 0.5 dropout in middle location\n",
            "Loading into a new model with 0.6000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.7000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.8 dropout in middle location\n",
            "Loading into a new model with 0.9 dropout in middle location\n",
            "Loading into a new model with 1.0 dropout in middle location\n",
            "the following model ResNet18.last.0.1 is starting from epoch : 50\n",
            "Current Loss 0.08803248811577213 \n",
            "Loading into a new model with 0.0 dropout in last location\n",
            "Loading into a new model with 0.1 dropout in last location\n",
            "Loading into a new model with 0.2 dropout in last location\n",
            "Loading into a new model with 0.30000000000000004 dropout in last location\n",
            "Loading into a new model with 0.4 dropout in last location\n",
            "Loading into a new model with 0.5 dropout in last location\n",
            "Loading into a new model with 0.6000000000000001 dropout in last location\n",
            "Loading into a new model with 0.7000000000000001 dropout in last location\n",
            "Loading into a new model with 0.8 dropout in last location\n",
            "Loading into a new model with 0.9 dropout in last location\n",
            "Loading into a new model with 1.0 dropout in last location\n",
            "the following model ResNet18.middle.0.1 is starting from epoch : 50\n",
            "Current Loss 0.08933918328366708 \n",
            "Loading into a new model with 0.0 dropout in middle location\n",
            "Loading into a new model with 0.1 dropout in middle location\n",
            "Loading into a new model with 0.2 dropout in middle location\n",
            "Loading into a new model with 0.30000000000000004 dropout in middle location\n",
            "Loading into a new model with 0.4 dropout in middle location\n",
            "Loading into a new model with 0.5 dropout in middle location\n",
            "Loading into a new model with 0.6000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.7000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.8 dropout in middle location\n",
            "Loading into a new model with 0.9 dropout in middle location\n",
            "Loading into a new model with 1.0 dropout in middle location\n",
            "the following model ResNet18.last.0.2 is starting from epoch : 50\n",
            "Current Loss 0.0908815360320732 \n",
            "Loading into a new model with 0.0 dropout in last location\n",
            "Loading into a new model with 0.1 dropout in last location\n",
            "Loading into a new model with 0.2 dropout in last location\n",
            "Loading into a new model with 0.30000000000000004 dropout in last location\n",
            "Loading into a new model with 0.4 dropout in last location\n",
            "Loading into a new model with 0.5 dropout in last location\n",
            "Loading into a new model with 0.6000000000000001 dropout in last location\n",
            "Loading into a new model with 0.7000000000000001 dropout in last location\n",
            "Loading into a new model with 0.8 dropout in last location\n",
            "Loading into a new model with 0.9 dropout in last location\n",
            "Loading into a new model with 1.0 dropout in last location\n",
            "the following model ResNet18.middle.0.2 is starting from epoch : 50\n",
            "Current Loss 0.09274989718338475 \n",
            "Loading into a new model with 0.0 dropout in middle location\n",
            "Loading into a new model with 0.1 dropout in middle location\n",
            "Loading into a new model with 0.2 dropout in middle location\n",
            "Loading into a new model with 0.30000000000000004 dropout in middle location\n",
            "Loading into a new model with 0.4 dropout in middle location\n",
            "Loading into a new model with 0.5 dropout in middle location\n",
            "Loading into a new model with 0.6000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.7000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.8 dropout in middle location\n",
            "Loading into a new model with 0.9 dropout in middle location\n",
            "Loading into a new model with 1.0 dropout in middle location\n",
            "the following model ResNet18.last.0.3 is starting from epoch : 50\n",
            "Current Loss 0.08779167255049106 \n",
            "Loading into a new model with 0.0 dropout in last location\n",
            "Loading into a new model with 0.1 dropout in last location\n",
            "Loading into a new model with 0.2 dropout in last location\n",
            "Loading into a new model with 0.30000000000000004 dropout in last location\n",
            "Loading into a new model with 0.4 dropout in last location\n",
            "Loading into a new model with 0.5 dropout in last location\n",
            "Loading into a new model with 0.6000000000000001 dropout in last location\n",
            "Loading into a new model with 0.7000000000000001 dropout in last location\n",
            "Loading into a new model with 0.8 dropout in last location\n",
            "Loading into a new model with 0.9 dropout in last location\n",
            "Loading into a new model with 1.0 dropout in last location\n",
            "the following model ResNet18.middle.0.3 is starting from epoch : 50\n",
            "Current Loss 0.09622895990138641 \n",
            "Loading into a new model with 0.0 dropout in middle location\n",
            "Loading into a new model with 0.1 dropout in middle location\n",
            "Loading into a new model with 0.2 dropout in middle location\n",
            "Loading into a new model with 0.30000000000000004 dropout in middle location\n",
            "Loading into a new model with 0.4 dropout in middle location\n",
            "Loading into a new model with 0.5 dropout in middle location\n",
            "Loading into a new model with 0.6000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.7000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.8 dropout in middle location\n",
            "Loading into a new model with 0.9 dropout in middle location\n",
            "Loading into a new model with 1.0 dropout in middle location\n",
            "the following model ResNet18.last.0.4 is starting from epoch : 50\n",
            "Current Loss 0.09494744360940531 \n",
            "Loading into a new model with 0.0 dropout in last location\n",
            "Loading into a new model with 0.1 dropout in last location\n",
            "Loading into a new model with 0.2 dropout in last location\n",
            "Loading into a new model with 0.30000000000000004 dropout in last location\n",
            "Loading into a new model with 0.4 dropout in last location\n",
            "Loading into a new model with 0.5 dropout in last location\n",
            "Loading into a new model with 0.6000000000000001 dropout in last location\n",
            "Loading into a new model with 0.7000000000000001 dropout in last location\n",
            "Loading into a new model with 0.8 dropout in last location\n",
            "Loading into a new model with 0.9 dropout in last location\n",
            "Loading into a new model with 1.0 dropout in last location\n",
            "the following model ResNet18.middle.0.4 is starting from epoch : 50\n",
            "Current Loss 0.1018545062301564 \n",
            "Loading into a new model with 0.0 dropout in middle location\n",
            "Loading into a new model with 0.1 dropout in middle location\n",
            "Loading into a new model with 0.2 dropout in middle location\n",
            "Loading into a new model with 0.30000000000000004 dropout in middle location\n",
            "Loading into a new model with 0.4 dropout in middle location\n",
            "Loading into a new model with 0.5 dropout in middle location\n",
            "Loading into a new model with 0.6000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.7000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.8 dropout in middle location\n",
            "Loading into a new model with 0.9 dropout in middle location\n",
            "Loading into a new model with 1.0 dropout in middle location\n",
            "the following model ResNet18.last.0.5 is starting from epoch : 50\n",
            "Current Loss 0.0966910229556961 \n",
            "Loading into a new model with 0.0 dropout in last location\n",
            "Loading into a new model with 0.1 dropout in last location\n",
            "Loading into a new model with 0.2 dropout in last location\n",
            "Loading into a new model with 0.30000000000000004 dropout in last location\n",
            "Loading into a new model with 0.4 dropout in last location\n",
            "Loading into a new model with 0.5 dropout in last location\n",
            "Loading into a new model with 0.6000000000000001 dropout in last location\n",
            "Loading into a new model with 0.7000000000000001 dropout in last location\n",
            "Loading into a new model with 0.8 dropout in last location\n",
            "Loading into a new model with 0.9 dropout in last location\n",
            "Loading into a new model with 1.0 dropout in last location\n",
            "the following model ResNet18.middle.0.5 is starting from epoch : 50\n",
            "Current Loss 0.09899016272434964 \n",
            "Loading into a new model with 0.0 dropout in middle location\n",
            "Loading into a new model with 0.1 dropout in middle location\n",
            "Loading into a new model with 0.2 dropout in middle location\n",
            "Loading into a new model with 0.30000000000000004 dropout in middle location\n",
            "Loading into a new model with 0.4 dropout in middle location\n",
            "Loading into a new model with 0.5 dropout in middle location\n",
            "Loading into a new model with 0.6000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.7000000000000001 dropout in middle location\n",
            "Loading into a new model with 0.8 dropout in middle location\n",
            "Loading into a new model with 0.9 dropout in middle location\n",
            "Loading into a new model with 1.0 dropout in middle location\n"
          ]
        }
      ],
      "source": [
        "for combination in combinations:\n",
        "  dropout_location, dropout_rate = combination\n",
        "  trial_dict_key = f\"{dropout_rate}_{dropout_location}\"\n",
        "\n",
        "  if trial_result_dict.get(trial_dict_key) is None:\n",
        "    trial_result_dict[trial_dict_key]={}\n",
        "\n",
        "  net, curr_epoch, train_loss_min = get_model_for_training(dropout_location, dropout_rate)\n",
        "  original_dict = copy.deepcopy(net.state_dict())\n",
        "  print(f\"Current Loss {train_loss_min} \")\n",
        "  for rate in np.arange(0, 1.1, 0.1):\n",
        "    print(f\"Loading into a new model with {rate} dropout in {dropout_location} location\")\n",
        "    net = ResNet18(rate, dropout_location)\n",
        "    net.load_state_dict(original_dict)\n",
        "\n",
        "    trial_test_key = f\"{rate}_test\"\n",
        "    trial_train_key = f\"{rate}_train\"\n",
        "\n",
        "    if trial_result_dict.get(trial_dict_key).get(trial_test_key) is None:\n",
        "      trial_result_dict[trial_dict_key][trial_test_key]= []\n",
        "    \n",
        "    if trial_result_dict.get(trial_dict_key).get(trial_train_key) is None:\n",
        "      trial_result_dict[trial_dict_key][trial_train_key] = []\n",
        "\n",
        "    start_idx = len(trial_result_dict[trial_dict_key][trial_train_key])\n",
        "\n",
        "    for trial in range(start_idx, TRIALS_NUMBER):  \n",
        "      print(f\"Current trial : {trial}\")\n",
        "      if train_on_gpu:\n",
        "        net = LoggerDataParallel(net, model_props = net.model_props)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "      # specify loss function (categorical cross-entropy)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # specify optimizer\n",
        "      optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "      #run_test_data(net, optimizer, criterion, False)\n",
        "      total_overall_accuracy_on_train = run_test_data(net, optimizer, criterion, True, True)\n",
        "      total_overall_accuracy_on_test = run_test_data(net, optimizer, criterion, True, False)\n",
        "\n",
        "      trial_result_dict[trial_dict_key][trial_test_key].append(total_overall_accuracy_on_test)\n",
        "      trial_result_dict[trial_dict_key][trial_train_key].append(total_overall_accuracy_on_train)\n",
        "      save_trials_dict(trial_result_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuRpFkiZANJn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FVwz1WydnN_7"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}